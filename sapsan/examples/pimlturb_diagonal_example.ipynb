{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Physics-Informed Convolutional model to predict\n",
    "the diagonal Reynolds stress tensor terms, which\n",
    "can later be used to calculate Turbulent Pressure.\n",
    "\n",
    "The example data is a sampled version of the 3D CCSN\n",
    "data used in P.I.Karpov, arXiv:2205.08663\n",
    "\n",
    "The algorithm is best suited to be trained on GPU/s,\n",
    "but for compatibility reasons the 'device' is set to 'cpu'\n",
    "in this example. Please change it if you have CUDA enabled system\n",
    "\n",
    "Note: due to a custom loss implementation, Catalyst is NOT used,\n",
    "hence logging is handled manually for this example!\n",
    "\n",
    "-pikarpov\n",
    "\"\"\"\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from sapsan.lib.backends import MLflowBackend, FakeBackend\n",
    "from sapsan.lib.data import HDF5Dataset, EquidistantSampling, flatten\n",
    "from sapsan.lib.estimator.pimlturb.pimlturb_diagonal_estimator import PIMLTurb, PIMLTurbConfig\n",
    "from sapsan.lib import Train, Evaluate\n",
    "from sapsan.utils.plot import log_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# --- Experiment tracking backend ---\n",
    "# MLflow - the server will be launched automatically\n",
    "# in case it won't, type in cmd: mlflow ui --port=5000\n",
    "# to use it, set mlflow = True\n",
    "\n",
    "mlflow = False\n",
    "experiment_name = \"PIMLTurb experiment\"\n",
    "\n",
    "if mlflow: \n",
    "    tracking_backend = MLflowBackend(experiment_name, host=\"localhost\", port=5000)\n",
    "else: tracking_backend = FakeBackend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# --- Data setup ---\n",
    "# In the intereset of loading and training multiple timesteps\n",
    "# one can specify which checkpoints to use and where\n",
    "# they appear in the path via syntax: {checkpoint:format}\n",
    "# \n",
    "# Next, you need to specify which features to load; let's assume \n",
    "#         path = \"{feature}.h5\"\n",
    "# \n",
    "#  1. If in different files, then specify features directly;\n",
    "#     The default HDF5 label will be the last label in the file\n",
    "#     Ex: features = ['velocity', 'denisty', 'pressure']\n",
    "#  2. If in the same, then duplicate the name in features\n",
    "#     and specify which labels to pull\n",
    "#     Ex: features = [\"data\", \"data\", \"data\"]\n",
    "#         feature_labels = ['velocity', 'density', 'pressure']\n",
    "\n",
    "sigma = 9\n",
    "base  = \"data/ccsn-mri-sims/\"\n",
    "path  = base + \"{feature}_dim17_s%d_it{checkpoint:1.0f}_float32.h5\"%(sigma)\n",
    "\n",
    "features = ['vel', 'Bvec', 'dvel', 'dBvec']\n",
    "target   = ['tnvel']\n",
    "\n",
    "# tensor component to train against, with the following layout:\n",
    "# 0  1  2\n",
    "# 3  4  5 \n",
    "# 6  7  8\n",
    "\n",
    "tn_comp = 0\n",
    "\n",
    "# Checkpoints to train on\n",
    "checkpoints = [40,60,80,100,120,140,160,180,200,220]\n",
    "\n",
    "# Dimensionality of your data in format [D,H,W]\n",
    "INPUT_SIZE = [17,17,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# train_fraction = len(checkpoints)-1) leaves 1 batch for validation\n",
    "\n",
    "data_loader = HDF5Dataset(path           = path,\n",
    "                          features       = features,\n",
    "                          target         = target,\n",
    "                          checkpoints    = checkpoints,\n",
    "                          input_size     = INPUT_SIZE,\n",
    "                          train_fraction = len(checkpoints)-1)\n",
    "\n",
    "x, y    = data_loader.load_numpy()\n",
    "y       = y[:, tn_comp:tn_comp+1]\n",
    "loaders = data_loader.convert_to_torch([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning model to use\n",
    "\n",
    "# Configuration of the model parameters:\n",
    "#    n_epochs  = number of epochs (need ~3000 for sensible results)\n",
    "#    patience  = number of epochs to run beyond convergence (not used for SmoothL1_KSLoss)\n",
    "#    min_delta = loss based convergence cut-off (not used for SmoothL1_KSLoss)\n",
    "#    lr        = learning rate\n",
    "#    min_lr    = minimum learning rate\n",
    "#    device    = device to train the model on: ['cpu', 'cuda', 'cuda:0', ...]\n",
    "#    activ     = activation function\n",
    "#    loss      = loss function\n",
    "#    ks_stop   = value of the KS (Kolmogorov-Smirnov stat) to stop training,\n",
    "#                checks both 'train' and 'valid' KS values \n",
    "#                Note: NOT the total loss value, but only the KS component!\n",
    "#    ks_frac   = contribution fraction of KS to the total loss. l1_frac = 1-ks_frac\n",
    "#    l1_scale  = amount to scale the initial L1 loss by, so that it is\n",
    "#                reduced first, with KS loss dominating later in the training\n",
    "#    ks_scale  = amount to scale the initial KS loss by (should always be = 1)\n",
    "#    l1_beta   = beta for the Smooth L1 (cutoff point for smooth portion)\n",
    "#    sigma     = sigma for the Gaussian kernel for the filtering step of each epoch\n",
    "\n",
    "estimator = PIMLTurb(\n",
    "        config   = PIMLTurbConfig(n_epochs  = 10, \n",
    "                                  patience  = 10, \n",
    "                                  min_delta = 1e-7, \n",
    "                                  lr        = 1e-4, \n",
    "                                  min_lr    = 1e-4*1e-5,\n",
    "                                  device    = 'cpu'),\n",
    "        loaders  = loaders,        \n",
    "        activ    = \"Tanhshrink\", \n",
    "        loss     = \"SmoothL1_KSLoss\",\n",
    "        ks_stop  = 0.04,\n",
    "        ks_frac  = 0.5,        \n",
    "        ks_scale = 1,\n",
    "        l1_scale = 1e6,    \n",
    "        l1_beta  = 1,\n",
    "        sigma    = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given the lower resolution, the published results cannot be achieved.\n",
    "# However, after ~1000 epochs, the predictions get quite good,\n",
    "# showing the effect of the custom loss.\n",
    "\n",
    "# Set the experiment\n",
    "training_experiment = Train(model           = estimator,\n",
    "                            backend         = tracking_backend,             \n",
    "                            data_parameters = data_loader)\n",
    "\n",
    "# Train the model\n",
    "estimator = training_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting training results\n",
    "# Note: not done automatically since Catalyst is NOT used\n",
    "log = log_plot(show_log       = True,\n",
    "               log_path       = 'train_log.txt', \n",
    "               valid_log_path = 'valid_log.txt', \n",
    "               delimiter      = '\\t')\n",
    "\n",
    "log_name = \"runtime_log.html\"\n",
    "log.write_html(log_name)\n",
    "tracking_backend.log_artifact(log_name)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# plotting individual evolutions of L1 and KS losses\n",
    "losses = [[\"mean(L1_loss)\", \"mean(L1_valid)\"],\n",
    "          [\"mean(KS_loss)\", \"mean(KS_valid)\"]]\n",
    "\n",
    "for idx, (train_name, valid_name) in enumerate(losses): \n",
    "    log = log_plot(show_log       = True, \n",
    "                   valid_log_path = 'valid_l1ks_log.txt',\n",
    "                   log_path       = 'train_l1ks_log.txt',\n",
    "                   delimiter      = '\\t',\n",
    "                   train_name     = train_name, \n",
    "                   valid_name     = valid_name,\n",
    "                   train_column   = idx, \n",
    "                   valid_column   = idx, \n",
    "                   epoch_column   = None)\n",
    "    \n",
    "    log_name = f\"{train_name}.html\"\n",
    "    log.write_html(log_name)    \n",
    "    tracking_backend.log_artifact(log_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "checkpoints_eval = [233,313,393]\n",
    "\n",
    "for chk in checkpoints_eval:\n",
    "    # Load the test data\n",
    "    data_loader_eval = HDF5Dataset(path        = path,\n",
    "                                   features    = features,\n",
    "                                   target      = target,\n",
    "                                   checkpoints = [chk],\n",
    "                                   input_size  = INPUT_SIZE,\n",
    "                                   )\n",
    "    test_x, test_y = data_loader_eval.load_numpy()\n",
    "    test_y         = test_y[:, tn_comp:tn_comp+1,::]\n",
    "    loaders_eval   = data_loader_eval.convert_to_torch([test_x, test_y])\n",
    "\n",
    "    # Set the test experiment\n",
    "    estimator.loaders = loaders_eval\n",
    "    evaluation_experiment = Evaluate(model           = estimator,\n",
    "                                     backend         = tracking_backend,\n",
    "                                     data_parameters = data_loader_eval,\n",
    "                                     pdf_xlim        = (-0.5e-3,1.5e-3),\n",
    "                                     cdf_xlim        = (-0.5e-3,1.5e-3))\n",
    "    \n",
    "    # Test the model\n",
    "    # Evaluation returns a dict, results = {'predict':np.ndarray, 'target':np.ndarray}\n",
    "    results = evaluation_experiment.run()\n",
    "    \n",
    "    # logs extra parameters\n",
    "    if mlflow:\n",
    "        run_id = evaluation_experiment.run_id\n",
    "        tracking_backend.resume(run_id = run_id)\n",
    "        tracking_backend.log_parameter(\"tn_comp\", tn_comp)    \n",
    "        tracking_backend.end()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('py310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "55761d48b13336894241222244b506485a2c10caa3a7f51cb6df6e994d5120a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
