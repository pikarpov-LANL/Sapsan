{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Physics-Informed Convolutional model to predict\n",
    "the diagonal Reynolds stress tensor terms, which\n",
    "can later be used to calculate Turbulent Pressure.\n",
    "\n",
    "The example data is a sampled version of the 3D CCSN\n",
    "data used in P.I.Karpov, arXiv:2205.08663\n",
    "\n",
    "The algorythm is best suited to be trained on GPU/s,\n",
    "but for compatibility reasons the 'device' is set to 'cpu'\n",
    "in this example. Please change it if you have CUDA enabled system\n",
    "\n",
    "Note: due to a custom loss implementation, Catalyst is NOT used,\n",
    "hence logging is handled manually for this example!\n",
    "\n",
    "-pikarpov\n",
    "'''\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from sapsan.lib.backends import MLflowBackend\n",
    "from sapsan.lib.data import HDF5Dataset, EquidistantSampling, flatten\n",
    "from sapsan.lib.estimator.pimlturb.pimlturb_diagonal_estimator import PIMLTurb, PIMLTurbConfig\n",
    "from sapsan import Train, Evaluate, model_graph\n",
    "from sapsan.utils.plot import log_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#--- Experiment tracking backend ---\n",
    "#MLflow - the server will be launched automatically\n",
    "#in case it won't, type in cmd: mlflow ui --port=9000\n",
    "#uncomment tracking_backend to use mlflow\n",
    "\n",
    "experiment_name = \"PIMLTurb experiment\"\n",
    "#tracking_backend = MLflowBackend(experiment_name, host=\"localhost\", port=9000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#--- Data setup ---\n",
    "#In the intereset of loading and training multiple timesteps\n",
    "#one can specify which checkpoints to use and where\n",
    "#they appear in the path via syntax: {checkpoint:format}\n",
    "#\n",
    "#Next, you need to specify which features to load; let's assume \n",
    "#        path = \"{feature}.h5\"\n",
    "#\n",
    "# 1) If in different files, then specify features directly;\n",
    "#    The default HDF5 label will be the last label in the file\n",
    "#    Ex: features = ['velocity', 'denisty', 'pressure']\n",
    "# 2) If in the same, then duplicate the name in features\n",
    "#    and specify which labels to pull\n",
    "#    Ex: features = [\"data\", \"data\", \"data\"]\n",
    "#        feature_labels = ['velocity', 'density', 'pressure']\n",
    "\n",
    "sigma = 9\n",
    "base = \"data/ccsn-mri-sims/\"\n",
    "path = base + \"{feature}_dim17_s%d_it{checkpoint:1.0f}.h5\"%(sigma)\n",
    "\n",
    "features = ['vel', 'Bvec', 'dvel', 'dBvec']\n",
    "target = ['tnvel']\n",
    "\n",
    "#tensor component to train against, with the following layout:\n",
    "# 0  1  2\n",
    "# 3  4  5 \n",
    "# 6  7  8\n",
    "\n",
    "tn_comp = 0\n",
    "\n",
    "#checkpoints to train on\n",
    "checkpoints = [40,60,80,100,120,140,160,180,200,220]\n",
    "\n",
    "#Dimensionality of your data in format [D,H,W]\n",
    "INPUT_SIZE = [17,17,17]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the data\n",
    "#train_fraction = len(checkpoints)-1) leaves 1 batch for validation\n",
    "\n",
    "data_loader = HDF5Dataset(path=path,\n",
    "                          features=features,\n",
    "                          target=target,\n",
    "                          checkpoints=checkpoints,\n",
    "                          input_size=INPUT_SIZE,\n",
    "                          train_fraction = len(checkpoints)-1)\n",
    "x, y = data_loader.load_numpy()\n",
    "\n",
    "y = y[:, tn_comp:tn_comp+1]\n",
    "\n",
    "loaders = data_loader.convert_to_torch([x, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning model to use\n",
    "\n",
    "#Configuration of the model parameters:\n",
    "#    n_epochs = number of epochs (need ~1000 for sensible results)\n",
    "#    patience = number of epochs to run beyond convergence\n",
    "#    min_delta = loss based convergence cut-off\n",
    "#    lr = learning rate\n",
    "#    min_lr = minimum learning rate\n",
    "#    device = device to train the model on: ['cpu', 'cuda', 'cuda:0', ...]\n",
    "#    activ = activation function\n",
    "#    loss = loss function\n",
    "#    ks_stop = at wait value of the KS (Kolmogorov-Smirnov stat) to stop training\n",
    "#              Note: NOT the total loss value, but only the KS component!\n",
    "#    scalel1 = by what amount to scale the initial L1 loss, so that it is\n",
    "#              reduced first, with KS loss dominating later in the training\n",
    "#    betal1 = beta for the Smooth L1 (cutoff point for smooth portion)\n",
    "#    sigma = sigma for the Gaussian kernel for the filtering step of each epoch\n",
    "\n",
    "estimator = PIMLTurb(\n",
    "        config = PIMLTurbConfig(n_epochs=10, patience=10, \n",
    "                                min_delta=1e-7, lr=1e-4, min_lr=1e-4*1e-5,\n",
    "                                device='cpu'),\n",
    "        loaders = loaders,        \n",
    "        activ=\"Tanhshrink\", loss=\"SmoothL1_KSLoss\",\n",
    "        ks_stop = 0.04,\n",
    "        scalel1 = 1e6,\n",
    "        betal1 = 1,\n",
    "        sigma = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Given the lower resolution, the published results cannot be achieved.\n",
    "#However, after ~1000 epochs, the predictions get quite good,\n",
    "#showing the effect of the custom loss.\n",
    "\n",
    "#Set the experiment\n",
    "training_experiment = Train(model = estimator,\n",
    "                            #backend=tracking_backend, #uncomment to use mlflow                            \n",
    "                            data_parameters = data_loader)\n",
    "\n",
    "#Train the model\n",
    "estimator = training_experiment.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting training results\n",
    "#Note: not done automatically since Catalyst is NOT used\n",
    "log = log_plot(show_log=True,\n",
    "               log_path = 'train_log.txt', \n",
    "               valid_log_path = 'valid_log.txt', \n",
    "               delimiter = '\\t')\n",
    "log_name = \"runtime_log.html\"\n",
    "log.write_html(log_name)\n",
    "#tracking_backend.log_artifact(log_name)\n",
    "\n",
    "#--------------------------------------------------\n",
    "#plotting individual evolutions of L1 and KS losses\n",
    "#Note: the individual losses are not logged at every epoch\n",
    "losses = [[\"mean(L1_loss)\", \"mean(L1_valid)\"],\n",
    "          [\"mean(KS_loss)\", \"mean(KS_valid)\"]]\n",
    "\n",
    "for idx, (train_name, valid_name) in enumerate(losses): \n",
    "    log = log_plot(show_log=True, \n",
    "                   valid_log_path = 'losses_valid_log.txt',\n",
    "                   log_path = 'losses_train_log.txt',\n",
    "                   delimiter = '\\t',\n",
    "                   train_name=train_name, valid_name=valid_name,\n",
    "                   train_column=idx, valid_column=idx, epoch_column=None)\n",
    "    log_name = f\"{train_name}.html\"\n",
    "    log.write_html(log_name)    \n",
    "    #tracking_backend.log_artifact(log_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "checkpoints_eval = [233,313,393]\n",
    "for chk in checkpoints_eval:\n",
    "    #Load the test data\n",
    "    data_loader_eval = HDF5Dataset(path=path,\n",
    "                       features=features,\n",
    "                       target=target,\n",
    "                       checkpoints=[chk],\n",
    "                       input_size=INPUT_SIZE,\n",
    "                       )\n",
    "    test_x, test_y = data_loader_eval.load_numpy()\n",
    "    test_y = test_y[:, tn_comp:tn_comp+1,::]\n",
    "    loaders_eval = data_loader_eval.convert_to_torch([test_x, test_y])\n",
    "\n",
    "    #Set the test experiment\n",
    "    estimator.loaders = loaders_eval\n",
    "    evaluation_experiment = Evaluate(model = estimator,\n",
    "                                     #backend = tracking_backend,\n",
    "                                     data_parameters = data_loader_eval,\n",
    "                                     pdf_xlim = (-0.5e-3,1.5e-3),\n",
    "                                     cdf_xlim = (-0.5e-3,1.5e-3))\n",
    "\n",
    "\n",
    "    #Test the model; get the dictionary of {'predict', 'target'}\n",
    "    prediction = evaluation_experiment.run()\n",
    "    \n",
    "    #logs extra parameters\n",
    "    #run_id = evaluation_experiment.run_id\n",
    "    #tracking_backend.resume(run_id = run_id)\n",
    "    #tracking_backend.log_parameter(\"tn_comp\", tn_comp)    \n",
    "    #tracking_backend.end()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
